{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7d025d373990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from dataset import Pic_to_Pic_dataset\n",
    "from models import UNET, U2NET\n",
    "from torch.utils.data import DataLoader\n",
    "from loss import SSIM_DICE_BCE, DiceScore\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image \n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import cv2\n",
    "from typing import List\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('rm -r plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "def get_depth_gen(length: int):\n",
    "    lst = list()\n",
    "    for i in range(length):\n",
    "        lst.extend([i]*10)\n",
    "    for depth in lst:\n",
    "        yield depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output): \n",
    "    batch_depth = next(depth_gen)\n",
    "    batch_size = output.shape[0]\n",
    "\n",
    "    for depth_, op in enumerate(output): \n",
    "        depth = str(batch_size*batch_depth+ depth_)\n",
    "        # print(depth)\n",
    "        for idx, img in enumerate(op): \n",
    "            path = os.path.join('plots', patient_id, depth, module.name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            idx = str(len(os.listdir(path)))\n",
    "            img = (img-img.min())/(img.max()-img.min())\n",
    "            # print(path)\n",
    "            ToPILImage()(img).save(path+ '/' + str(idx) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning names to layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7d036d0e8390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNET().to('cuda:1')\n",
    "model.assign_names()\n",
    "ckpt = torch.load('./ckpts/quantum_noise/56/best_unet.pth') \n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "dice_score = DiceScore()\n",
    "# print(model)\n",
    "\n",
    "\n",
    "model.ch.register_forward_hook(forward_hook)\n",
    "model.down1.register_forward_hook(forward_hook)\n",
    "model.down2.register_forward_hook(forward_hook)\n",
    "model.down3.register_forward_hook(forward_hook)\n",
    "model.down4.register_forward_hook(forward_hook)\n",
    "model.up1.register_forward_hook(forward_hook)\n",
    "model.up2.register_forward_hook(forward_hook)\n",
    "model.up3.register_forward_hook(forward_hook)\n",
    "model.up4.register_forward_hook(forward_hook)\n",
    "model.out.register_forward_hook(forward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_vid(path): \n",
    "    imgs = os.listdir(path)\n",
    "    imgs = [os.path.join(path, img) for img in imgs if img.endswith('.png')]\n",
    "    imgs = sorted(imgs, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "    assert len(imgs) > 0, 'No images found in {}'.format(path)\n",
    "    img = cv2.imread(imgs[0])\n",
    "    video = cv2.VideoWriter('{}/vid.mp4'.format(path), cv2.VideoWriter_fourcc(*'mp4v'), 10, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    for img_path in tqdm(imgs): \n",
    "        if not img_path.endswith('.png'): \n",
    "            continue\n",
    "        img = cv2.imread(img_path)\n",
    "        video.write(img)       \n",
    "    video.release()\n",
    "    os.system('rm path + *.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]/home/shivac/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      " 32%|███▏      | 10/31 [06:26<13:37, 38.94s/it]"
     ]
    }
   ],
   "source": [
    "whole_df = pd.read_csv('/home/shivac/qml-data/csv_files/val_10_org.csv') \n",
    "patient_ids = whole_df['patient_id'].unique()\n",
    "patient_id = np.random.choice(patient_ids)\n",
    "global batch_num \n",
    "batch_num = 0\n",
    "img_paths = whole_df[whole_df['patient_id'] == patient_id]['img_path'].values\n",
    "img_paths = [os.path.join('/home/shivac/qml-data/', img_path) for img_path in img_paths]\n",
    "batch_img = list() \n",
    "for img_path in img_paths: \n",
    "    img = Image.open(img_path).convert('L') \n",
    "    img = ToTensor()(img)\n",
    "    batch_img.append(img)\n",
    "batch_img = torch.stack(batch_img)\n",
    "batch_size = 10\n",
    "batch_img = batch_img.split(batch_size)\n",
    "depth_gen = get_depth_gen(len(img_paths)//batch_size)\n",
    "print(f'{len(batch_img)}')\n",
    "with torch.no_grad():\n",
    "    for img in tqdm(batch_img): \n",
    "        logits = model(img.to('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = 'plots/'\n",
    "patient_dirs = os.listdir(plots_path) \n",
    "for patient_dir in patient_dirs:\n",
    "    patient_dir_path = os.path.join(plots_path, patient_dir)\n",
    "    for depth in tqdm(os.listdir(patient_dir_path)): \n",
    "        depth_path = os.path.join(patient_dir_path, depth)\n",
    "        for module in os.listdir(depth_path): \n",
    "            module_path = os.path.join(depth_path, module)\n",
    "            imgs_to_vid(module_path)\n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/301 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivac/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      " 31%|███       | 92/301 [04:47<10:53,  3.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m ToTensor()(mask)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m img \u001b[38;5;241m=\u001b[39m ToTensor()(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(img\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m     18\u001b[0m dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(dice_score(mask\u001b[38;5;241m.\u001b[39mcuda(), logits)\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m), facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/qml-mn/Quantum/models/unet.py:131\u001b[0m, in \u001b[0;36mUNET.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x1)\n\u001b[1;32m    130\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown3(x2)\n\u001b[0;32m--> 131\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown4(x3)\n\u001b[1;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x, x3)\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x2)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, result)\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mforward_hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m (img\u001b[38;5;241m-\u001b[39mimg\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;241m/\u001b[39m(img\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m-\u001b[39mimg\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m----> 8\u001b[0m ToPILImage()(img)\u001b[38;5;241m.\u001b[39msave(path\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(idx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_pil_image(pic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:324\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(npimg, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3154\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3152\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, rawmode, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3060\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3058\u001b[0m     args \u001b[38;5;241m=\u001b[39m mode, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _MAPMODES:\n\u001b[0;32m-> 3060\u001b[0m     im \u001b[38;5;241m=\u001b[39m new(mode, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m   3061\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_new(core\u001b[38;5;241m.\u001b[39mmap_buffer(data, size, decoder_name, \u001b[38;5;241m0\u001b[39m, args))\n\u001b[1;32m   3062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:2974\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2972\u001b[0m     im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m ImagePalette\u001b[38;5;241m.\u001b[39mImagePalette()\n\u001b[1;32m   2973\u001b[0m     color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color)\n\u001b[0;32m-> 2974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(core\u001b[38;5;241m.\u001b[39mfill(mode, size, color))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "whole_df = pd.read_csv('/home/shivac/qml-data/csv_files/val_10_org.csv') \n",
    "patient_ids = np.unique(whole_df.patient_id)\n",
    "model.eval()\n",
    "for patient_id in patient_ids:\n",
    "    os.makedirs('plots/{}'.format(patient_id), exist_ok=True)\n",
    "    df = whole_df[whole_df.patient_id == patient_id].sort_values('idx')\n",
    "    df.reset_index(inplace=True)\n",
    "    path_gen = path_gen_fn(['plots/{}/{}/'.format(patient_id, i) for i in range(len(df))])\n",
    "    for i in tqdm(range(len(df))): \n",
    "        os.makedirs('plots/{}/{}'.format(patient_id, i), exist_ok=True)\n",
    "        img_path = '/home/shivac/qml-data/' + df.loc[i].img_path\n",
    "        mask_path = '/home/shivac/qml-data/' + df.loc[i].mask_path\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        mask = Image.open(mask_path) \n",
    "        mask = ToTensor()(mask).unsqueeze(0)\n",
    "        img = ToTensor()(img).unsqueeze(0)\n",
    "        logits = model(img.cuda())\n",
    "        dice = round(dice_score(mask.cuda(), logits).item(), 2)\n",
    "\n",
    "        plt.figure(figsize=(10, 6), facecolor='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Depth: ' + str(i) + ' dice_score: ' + str(dice))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title('img')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[0].permute(1,2,0), cmap='gray')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title('mask')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(mask[0].permute(1,2,0), cmap='gray')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.title('logits')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(logits[0].detach().cpu().permute(1,2,0), cmap='gray')\n",
    "        plt.savefig('plots/{}/{}/res.png'.format(patient_id, i))\n",
    "        plt.clf() \n",
    "        plt.close()\n",
    "        for path in os.listidir('plots/{}/{}'.format(patient_id, i)):\n",
    "            imgs_to_vid(os.path.join(path, 'ch/'))\n",
    "    break\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MEDVID0076_M_20211124_103754_0001_IMAGES'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
